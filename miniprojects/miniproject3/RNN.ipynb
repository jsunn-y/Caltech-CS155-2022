{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#all possible character tokens\n",
    "all_letters = string.ascii_letters + \" .,;:?!'-()\\n\"\n",
    "all_letters_list = list(all_letters)\n",
    "n_letters = len(all_letters) + 1\n",
    "\n",
    "#hyperparameters\n",
    "sequence_length = 40 #length of the sequence to learn from\n",
    "offset = 1 #offset when drawing from the full text file, 1 means dense\n",
    "\n",
    "next_length = 1 #length of sequence to predict\n",
    "next_offset = 1 #how shifted the predicted sequence is forward\n",
    "\n",
    "input_size = n_letters\n",
    "hidden_size = 200\n",
    "num_classes = n_letters\n",
    "num_layers = 1\n",
    "batch_size = 128\n",
    "num_epochs = 150\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adapted from https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "def readLines(filename):\n",
    "    with open(filename, encoding='utf-8') as some_file:\n",
    "        corpus = some_file.read()#.replace('\\n', ' ')\n",
    "        corpus = ''.join([i for i in corpus if not i.isdigit()])\n",
    "        return corpus\n",
    "\n",
    "def generate_sequences(filename, sequence_length=sequence_length, offset=offset, next_length=next_length):\n",
    "    '''\n",
    "    loads in the training data from a .txt file and preprocesses into 40-character chunks\n",
    "    filename: name of the .txt file containing shakespearan sonnets\n",
    "    length: the sequence_length used by the RNN\n",
    "    offset: the offset between each sequence window, use offset=1 for the densent sequences\n",
    "    '''\n",
    "    corpus = readLines(filename)\n",
    "\n",
    "    N = len(corpus)\n",
    "    sequences = []\n",
    "    next = []\n",
    "    index = 0\n",
    "    while index + sequence_length + 1 < N:\n",
    "        sequences.append(corpus[index: index + sequence_length])\n",
    "        next.append(corpus[index + sequence_length + next_offset -next_length: index + sequence_length + next_offset])\n",
    "        index += offset\n",
    "\n",
    "    return sequences, next\n",
    "\n",
    "def inputTensor(sequences, length = sequence_length):\n",
    "    '''\n",
    "    Generates a tensor that contains one-hot matrices for charcter sequences.\n",
    "    This is the correct format for the X inputs.\n",
    "    '''\n",
    "    tensor = torch.zeros(len(sequences), length, n_letters)\n",
    "    for (i, seq) in enumerate(sequences):\n",
    "        for li in range(length):\n",
    "            letter = seq[li]\n",
    "            tensor[i][li][all_letters.find(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "def categoryTensor(sequences, next_length=next_length):\n",
    "    '''\n",
    "    Generates a tensor that contains the token for each character.\n",
    "    This is the correct format for the y labels.\n",
    "    '''\n",
    "    tokenized = torch.zeros(len(sequences), next_length)\n",
    "    for (i, seq) in enumerate(sequences):\n",
    "        for li in range(next_length):\n",
    "            letter = seq[li]\n",
    "        tokenized[i][li] = all_letters_list.index(letter)\n",
    "    # tensor = torch.zeros(1, n_letters)\n",
    "    # tensor[0][li] = 1\n",
    "    # return tensor\n",
    "    return tokenized.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and process the data for training\n",
    "sequences, next = generate_sequences('data/shakespeare.txt')\n",
    "X = inputTensor(sequences)\n",
    "y = categoryTensor(next)\n",
    "\n",
    "train_dataset = TensorDataset(X, y)\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                                batch_size=batch_size, \n",
    "                                shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97634"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    '''\n",
    "    Class for the RNN model in pytorch\n",
    "    '''\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes) #next_length*\n",
    "        #no softmax needed since it is calculated in the cross entropy\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states \n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        x, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        \n",
    "        #select the length of the sequence that should be predicted\n",
    "        x = x[:, -next_length:, :]\n",
    "        #stack the sequence \n",
    "        out = x.reshape(x.size()[0]*x.size()[1], self.hidden_size)\n",
    "\n",
    "        # Decode the hidden state of all of the timesteps\n",
    "        #take all the length of the sequence you want to predict\n",
    "        out = self.fc(out) \n",
    "\n",
    "        #resahpe the array so that it is in the format for cross entropy loss\n",
    "        out = out.reshape(x.size()[0], x.size()[1], -1)\n",
    "        out = torch.swapaxes(out, 1, 2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    '''\n",
    "    Trains a RNN given the hyparameters given above\n",
    "    '''\n",
    "    model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Train the model\n",
    "    total_step = len(train_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        cum_loss = 0\n",
    "        for i, (X, y) in enumerate(train_loader):\n",
    "            #X = X.reshape(-1, input_size, sequence_length).to(device)\n",
    "            #X = X.reshape(-1, sequence_length, input_size).to(device)\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(X)\n",
    "            #print(outputs.shape)\n",
    "            #print(y.shape)\n",
    "            loss = criterion(outputs, y)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            cum_loss += loss\n",
    "        \n",
    "        if (epoch) % 10 == 0:\n",
    "            print ('Epoch [{}/{}], Loss: {:.4f}' \n",
    "                    .format(epoch+1, num_epochs, cum_loss.item()))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def sample(model, temperature, start_seq=[\"shall i compare thee to a summer's day?\\n\"], max_length = 560):\n",
    "    '''\n",
    "    Generates a poem\n",
    "    model: trained RNN model\n",
    "    start_seq: a 40 character starting seed for the poem generation\n",
    "    max_length: how many characters to generate\n",
    "    '''\n",
    "\n",
    "    with torch.no_grad():  # no need to track history in sampling\n",
    "        input = inputTensor(start_seq)\n",
    "        output_seq = start_seq[0]\n",
    "        np.random.RandomState(42)\n",
    "\n",
    "        for i in range(max_length):\n",
    "            input = input.to(device)\n",
    "            output = model(input)\n",
    "            softmax = nn.Softmax(dim=1)\n",
    "            \n",
    "            #deterministic sampling, take the most likely letter\n",
    "            if temperature == 0:\n",
    "                topvs, topis = output.topk(1, dim = 1)\n",
    "                #print(topis.shape)\n",
    "                for i in range(next_offset):\n",
    "                    topi = topis[0][0][next_length - 1 + i]\n",
    "                    letter = all_letters[topi]   \n",
    "                    output_seq += letter\n",
    "            \n",
    "            #probabilisitc sampling based on a temperature parameter  \n",
    "            else:\n",
    "                output /= temperature\n",
    "                output = softmax(output)\n",
    "                probs = (output[0,:,-1]).cpu()\n",
    "                #for numerical stability\n",
    "                probs /= (probs.sum()+1e-5)\n",
    "                \n",
    "                for i in range(next_offset):\n",
    "                    indices = range(n_letters)\n",
    "                    index = np.argmax(np.random.multinomial(1, probs))\n",
    "                    #index = np.random.choice(indices, p=probs)\n",
    "                    letter = all_letters_list[index]\n",
    "                    output_seq += letter\n",
    "        \n",
    "            input = inputTensor([output_seq[-40:]])\n",
    "\n",
    "        return output_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Loss: 2025.5798\n",
      "Epoch [11/150], Loss: 1140.8790\n",
      "Epoch [21/150], Loss: 922.6141\n",
      "Epoch [31/150], Loss: 719.8034\n",
      "Epoch [41/150], Loss: 569.3052\n",
      "Epoch [51/150], Loss: 472.6846\n",
      "Epoch [61/150], Loss: 409.8349\n",
      "Epoch [71/150], Loss: 367.9156\n",
      "Epoch [81/150], Loss: 334.7524\n",
      "Epoch [91/150], Loss: 307.0707\n",
      "Epoch [101/150], Loss: 279.2007\n",
      "Epoch [111/150], Loss: 262.9477\n",
      "Epoch [121/150], Loss: 248.2036\n",
      "Epoch [131/150], Loss: 232.6705\n",
      "Epoch [141/150], Loss: 231.0606\n"
     ]
    }
   ],
   "source": [
    "model = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shall i compare thee to a summer's day?\n",
      "Thou art more loves to the very abrous fidle,\n",
      "And yet not still even his black with mune.\n",
      "  In thy sweet formering of their world expues,\n",
      "For that say more that which fline in seem,\n",
      "And say in his beauty shall to thee doth live,\n",
      "  And since honour more than thou souldst in me,\n",
      "Hine eyes, your lous'st me not so much rest\n",
      "Onfeiticy, but by thy self with pure han eres.\n",
      "\n",
      "\n",
      "                   \n",
      "Then stoply fair that my time that case sone.\n",
      "  To tiully me to make our name, she less\n",
      "Is wond of mine on shamonous a many.\n",
      "Naturay drien presage of thee despised,\n",
      "But \n"
     ]
    }
   ],
   "source": [
    "print(sample(model, temperature = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shall i compare thee to a summer's day?\n",
      "Thou art me, thou should thou wilt besmoun,\n",
      "As those lipse that this shall gave my heart'st burt,\n",
      "And so my plays where my love as your lies,\n",
      "And so by these vainted acter is bend,\n",
      "That for this thought goner, which is so greater,\n",
      "But not then thou my love tame, her the sun\n",
      " outs do turn for my says our presict triff\n",
      "With leathe her to thee requite upon brow?\n",
      "Why of those would beauge we lack upon thethted,\n",
      "Mine own truth voights which to ensuray peeds.\n",
      "  You whom she is near suity to steep to be.\n",
      "Af of hunot after that I in thy art?\n",
      "Or cally of poor and\n"
     ]
    }
   ],
   "source": [
    "print(sample(model, temperature = 0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shall i compare thee to a summer's day?\n",
      "Thou art more bonds in their murity sight,\n",
      "Ditied by hister-blace eachers wretch doth lack,\n",
      "Whon of love that terges to every hade,\n",
      "  So those that which thou restaling so vend.\n",
      "If thou most love to whose rendos excelleghed:\n",
      "A trave alter in all alone shall tone's past.\n",
      "Letthenoud, bluck intermita inds dow:\n",
      "  To sourkent thou the least, some false old,\n",
      "And keeps thee I have still the store,\n",
      "  But sweets of thee, 'Nat this auris affooted.\n",
      "\n",
      "\n",
      "                   \n",
      "I ne' from me for dowh hell with d-ain heece,\n",
      "And by that I my body the contenced,\n",
      "And but a fil\n"
     ]
    }
   ],
   "source": [
    "print(sample(model, temperature = 0.75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shall i compare thee to a summer's day?\n",
      "How have eye knoulgly detyoly chadly lien,\n",
      "To play as for hore, why when that you should po most can,\n",
      "He capenury cantel fronds be recods,\n",
      "Or can uper canngry in other bold,\n",
      "Thy objeess wat as your sweet as doth,\n",
      "Unter theme leach unless to sleep, there graines.\n",
      "\n",
      "\n",
      "                    \n",
      "How makenemant, more int'ring with mancest,\n",
      "The clear besteet forly sleak extile hey,\n",
      "Goor better it were beremone) shall wond colfaded note.\n",
      "  Orieviol, whoch welldingums devosed view,\n",
      "Yet I this, manying no, and my love swear,\n",
      "Thy should kind, but they with thy altaints s\n"
     ]
    }
   ],
   "source": [
    "print(sample(model, temperature = 1.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "06d12861cad82bdcdde1b56bd9eda52e91f7df29dabbeda8f3d9112222750302"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('main_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
